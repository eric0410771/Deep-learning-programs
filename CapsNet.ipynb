{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUIXHH7F8IJN",
        "colab_type": "text"
      },
      "source": [
        "<a href = \"https://zhuanlan.zhihu.com/p/32156167\">Capsule Network Tutorial</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0SzknEsQMlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets,transforms\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available:\n",
        "    device = torch.device('cuda')\n",
        "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GHN9qan7TJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Mnist:\n",
        "    def __init__(self,batch_size):\n",
        "        dataset_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.,), (1.0,))\n",
        "        ])\n",
        "        \n",
        "        train_data = datasets.MNIST('../data', train = True, download = True, transform = dataset_transform)\n",
        "        test_data = datasets.MNIST('../data',train = False, download = True,transform = dataset_transform)\n",
        "        self.train_loader = torch.utils.data.DataLoader(train_data,batch_size = batch_size,shuffle = True)\n",
        "        self.test_loader = torch.utils.data.DataLoader(test_data,batch_size = batch_size,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tcdiL_ZTxYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvLayer(nn.Module):\n",
        "    def __init__(self,in_channel = 1, out_channel = 256,kernel_size = 9,stride = 1,padding = 0):\n",
        "        super(ConvLayer,self).__init__()\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels = in_channel,out_channels = out_channel,kernel_size = kernel_size, stride = stride,padding = padding)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.conv(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKPVMcCLch7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrimaryCapsule(nn.Module):\n",
        "    def __init__(self,num_capsule = 8, in_channel = 256, out_channel = 32,kernel_size = 9, stride = 2):\n",
        "        super(PrimaryCapsule,self).__init__()\n",
        "        self.num_capsule = num_capsule\n",
        "        self.capsules = nn.ModuleList([nn.Conv2d(in_channels = in_channel, out_channels = out_channel, kernel_size = kernel_size,stride = stride) for i in range(num_capsule)])\n",
        "    def forward(self,x):\n",
        "        out = [capsule(x) for capsule in self.capsules]\n",
        "        out = torch.stack(out,dim = 1)\n",
        "        out = out.view(out.size()[0],-1,self.num_capsule)\n",
        "        return self.squash(out)\n",
        "      \n",
        "    def squash(self,x):\n",
        "        square_x = (x ** 2).sum(-1,keepdim = True)\n",
        "        x_ = (square_x * x) / ((1+square_x) * torch.sqrt(square_x))\n",
        "        return x_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmpJEHTXhIU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DigitCapsule(nn.Module):\n",
        "    def __init__(self,num_route = 1152,num_capsule = 10,input_channel = 8, output_channel = 16,routing_iter = 3):\n",
        "        super(DigitCapsule,self).__init__()\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.num_capsule = num_capsule\n",
        "        self.num_route = num_route\n",
        "        \n",
        "        self.routing_iter = routing_iter\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "        self.w = nn.Parameter(torch.randn(1,num_route,num_capsule, output_channel, input_channel))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        x = torch.stack([x] * self.num_capsule,dim = 2).unsqueeze(4)\n",
        "        '''\n",
        "            num_route 其實是 input_capsule.\n",
        "            那首先這裡我們可以看到原本的neural network中， input shape通常是(batch_size, num_neuron)，W = (num_neuron,new_num_neuron)，\n",
        "            那這裡就可以看到在capsule中，其input_shape會是(batch_size,num_capsule,dimension)，而在capsule的W中會需要有新的new_num_capsule,和new_dimension，\n",
        "            ，而因為程式碼在進行矩陣相乘時，其前面dim - 2的shape都要相同，例如input shape如果是(batch_size,num_capsule,num_dim)\n",
        "            而W是(new_num_capsule,new_dimension_old_dimension)，而為了能進行matmul其共同就要擁有batch_size, num_capsule, new_capsule, dimension, new_dimension.\n",
        "            \n",
        "            \n",
        "            這個部分將x重複output所有的capsule次數，所以其shape會變成(batch_size, num_route, output_capsule,new_dimension, olddimension)\n",
        "            ，那我們可以想一下為什麼要重複output的capsule的數目，這裡我們知道在input_capsule -> output_capsule時，每個output_capsule其實都會看到\n",
        "            一組完整的input_capsule，而因為在創建W時，一定會有route * output_capsule的數量，而為了使W與x在做相乘時，是能直接用torch.matmul的，\n",
        "            所以讓該x重複多次，而最終的unsqueeze(4)也是為了讓matmul能夠進行\n",
        "            \n",
        "            我們看W的shape是 (batch_size, route, output_capsule,new_dimension,dimension)\n",
        "        '''\n",
        "        \n",
        "        w = torch.cat([self.w] * batch_size,dim = 0)\n",
        "        ## shape of x is (batch_size, input_capsule, output_capsule,input_dimension,1)\n",
        "        ## shape of w is (batch_size, input_capsule, output_capsule, output_dimension ,input_dimension)\n",
        "        u_hat = torch.matmul(w,x)\n",
        "        \n",
        "        ## shape of u_hat is (batch_size, input_capsule, output_channel ,output_dimension, 1)\n",
        "        \n",
        "        bij = torch.zeros(1,self.num_route, self.num_capsule,1)\n",
        "        ##那此部分比較能知道bij就是要有 input_capsule * output_channel 的 shape ，其中前後 1 , ... ,1是為了使程式碼能進行。\n",
        "        ##因為要進行相乘需要有相同或是較少但前面有的shape，例如(1,2,3,1)就可以和(1,2,3,2,2)，進行相乘，而這前提是前三個dim相同，而最後1，\n",
        "        ##則被視為一個scalar所以就沒差。\n",
        "        \n",
        "        for i in range(self.routing_iter):\n",
        "        \n",
        "            cij = self.softmax(bij)\n",
        "        \n",
        "            cij = torch.cat([cij] * batch_size,dim = 0).unsqueeze(4)\n",
        "            \n",
        "            sj = torch.sum((cij * u_hat),dim = 1,keepdim = True)\n",
        "            sj = sj.squeeze(4)\n",
        "            vj = self.squash(sj).unsqueeze(4)\n",
        "            #vj = self.squash(sj)\n",
        "            ## shape of vj is (batch_size, 1, output_capsule,output_channel, 1)\n",
        "            ## shape of u_hat is (batch_size, input_capsule, output_capsule, output_channel, 1)\n",
        "            aij = torch.matmul(u_hat.transpose(3,4),torch.cat([vj] * self.num_route, dim = 1)).mean(0,keepdim = True).squeeze(4)\n",
        "            if i < self.routing_iter - 1:\n",
        "                bij = aij + bij\n",
        "            ## shape of aij is (batch_size, input_capsule, output_capsule, 1 , 1)\n",
        "        return vj.squeeze(1)\n",
        "    def squash(self,x):\n",
        "        square_x = (x ** 2).sum(dim = -1,keepdim = True)\n",
        "        output = (square_x * x)/((1+square_x) * torch.sqrt(square_x))\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pT8njphdfJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.reconstruction_layer = nn.Sequential(\n",
        "            nn.Linear(16*10,512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256,784),\n",
        "            nn.Sigmoid()\n",
        "            \n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "    def forward(self, x, data):\n",
        "        classes = torch.sqrt((x ** 2).sum(2)) ## 此處是為了得到這個每筆資料對每個class的vector length \n",
        "        ## shape of classes is (batch_size, 10, 1)\n",
        "        classes = self.softmax(classes)\n",
        "        value, index = classes.max(dim = 1)\n",
        "        \n",
        "        masked = torch.eye(10)\n",
        "        \n",
        "        masked = masked.index_select(dim = 0, index = index.squeeze(1).data)\n",
        "        \n",
        "        reconstruction = self.reconstruction_layer( (x * masked[:,:,None,None]).view(x.size()[0], -1))\n",
        "        reconstruction = reconstruction.view(reconstruction.size()[0], 1, 28, 28)\n",
        "        \n",
        "        return reconstruction, masked \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mtm7MUPPv6u-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CapsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CapsNet,self).__init__()\n",
        "        self.conv = ConvLayer()\n",
        "        self.pricap = PrimaryCapsule()\n",
        "        self.digcap = DigitCapsule()\n",
        "        self.decoder = Decoder()\n",
        "        self.MSELoss = nn.MSELoss()\n",
        "        self.discount = 5e-4\n",
        "        \n",
        "    def forward(self,x):\n",
        "        conv_out = self.conv(x)\n",
        "        pricap_out = self.pricap(conv_out)\n",
        "        digcap_out = self.digcap(pricap_out)\n",
        "        reconstruction, masked = self.decoder(digcap_out, x)\n",
        "        \n",
        "        return digcap_out, reconstruction, masked \n",
        "      \n",
        "    def loss(self, data, x, target, reconstructions):\n",
        "        return self.margin_loss(x, target) + self.reconstruction_loss(data, reconstructions)\n",
        "      \n",
        "    def margin_loss(self,digcap_out,label):\n",
        "        \n",
        "        batch_size = digcap_out.size()[0]\n",
        "        relu = nn.ReLU()\n",
        "        \n",
        "        vector_length = torch.sqrt((digcap_out ** 2).sum(2))\n",
        "        \n",
        "        left_loss = relu(0.9 - vector_length).view(batch_size, -1)\n",
        "        right_loss = relu(vector_length - 0.1).view(batch_size, -1)\n",
        "\n",
        "        loss = label * left_loss +0.5 * (1.0 - label) * right_loss\n",
        "        return loss.sum(dim = 1).mean()\n",
        "        \n",
        "        ##shape of vector_length is (batch_size,output_capsule,1)\n",
        "    def reconstruction_loss(self, predict_x, x):\n",
        "        return self.MSELoss(predict_x.view(predict_x.size()[0],-1), x.view(x.size()[0],-1)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtYM8aFIU4Le",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "abe93490-bc81-432e-a56f-77ece95c4a68"
      },
      "source": [
        "capsnet = CapsNet()\n",
        "capsnet.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CapsNet(\n",
              "  (conv): ConvLayer(\n",
              "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
              "    (relu): ReLU()\n",
              "  )\n",
              "  (pricap): PrimaryCapsule(\n",
              "    (capsules): ModuleList(\n",
              "      (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (1): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (2): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (3): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (4): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (5): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (6): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "      (7): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
              "    )\n",
              "  )\n",
              "  (digcap): DigitCapsule(\n",
              "    (softmax): Softmax()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (reconstruction_layer): Sequential(\n",
              "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=256, out_features=784, bias=True)\n",
              "      (5): Sigmoid()\n",
              "    )\n",
              "    (softmax): Softmax()\n",
              "  )\n",
              "  (MSELoss): MSELoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOlIUW4FXJWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_result(reconstruction_images, images):\n",
        "    random_index = np.random.choice(len(reconstruction_images),5)\n",
        "    reconstruction_images = reconstruction_images[random_index]\n",
        "    images = images[random_index]\n",
        "    reconstruction_images = reconstruction_images.squeeze()\n",
        "    images = images.squeeze()\n",
        "    #reconstruction_images = reconstruction_images.transpose((0,2,3,1))\n",
        "    #images = images.transpose((0,2,3,1))\n",
        "    \n",
        "    plt.subplots(1,5)\n",
        "    plt.title('Real')\n",
        "    for i, image in enumerate(images):\n",
        "        \n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "    \n",
        "    plt.subplots(1,5)\n",
        "    plt.title('Predict')\n",
        "    for i, image in enumerate(reconstruction_images):\n",
        "        \n",
        "        plt.subplot(1, 5, i+1)\n",
        "        plt.imshow(image)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgURahd9F2dS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 1\n",
        "adam = optim.Adam(capsnet.parameters())\n",
        "#adam2 = optim.Adam(capsnet2.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKLSNK-dbHRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = Mnist(100)\n",
        "for epoch in range(epochs):\n",
        "    capsnet.train()\n",
        "    for i, element in enumerate(mnist.train_loader):\n",
        "        data, target = element\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        target = torch.eye(10).index_select(dim = 0,index = target)\n",
        "        \n",
        "        digcap_out, reconstruction, masked = capsnet(data)\n",
        "        loss = capsnet.loss(data,digcap_out,target,reconstruction)\n",
        "        \n",
        "        adam.zero_grad()\n",
        "        loss.backward()\n",
        "        adam.step()\n",
        "        if i % 10 == 0:\n",
        "            print(loss)\n",
        "            print(\"train accuracy:\", sum(np.argmax(masked.cpu().detach().numpy(),1) == np.argmax(target.cpu().detach().numpy())) / float(30))\n",
        "            plot_result(reconstruction.cpu().detach().numpy(),data.cpu().detach().numpy())\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECW8PF6IbOvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensor = torch.randn(1,5)\n",
        "print(tensor.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRAlBLKtu2H3",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7lEYZZWPPQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tensor.sum(-1,keepdim = True))\n",
        "print(tensor.sum(-1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_zgISnYPT9i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f9192270-e083-42bf-ea42-c1814fa68436"
      },
      "source": [
        "print(tensor/tensor.sum(-1,keepdim = True))\n",
        "print(tensor/tensor.sum(-1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.2307, -1.9394,  1.1864,  1.1580, -1.6357]])\n",
            "tensor([[ 2.2307, -1.9394,  1.1864,  1.1580, -1.6357]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwLP6fBQPtXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tensor = torch.zeros(1,10,2,1)\n",
        "softmax = nn.Softmax()\n",
        "output_tensor = softmax(test_tensor)\n",
        "output_tensor2 = F.softmax(test_tensor)\n",
        "\n",
        "print(output_tensor.size())\n",
        "print(output_tensor)\n",
        "print(output_tensor2.size())\n",
        "print(output_tensor2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5NBz39JeOkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7e28047-368c-47ef-d067-2ca7fa418e45"
      },
      "source": [
        "tensor = torch.tensor([[[2.0],[3.0]]])\n",
        "print(tensor.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfXbUVNKqee0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tensor = torch.ones(1,2,5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzfntNlLqmQs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "725bec19-7530-46a3-ac30-562d1d22ced3"
      },
      "source": [
        "print(tensor * test_tensor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[2., 2., 2., 2., 2.],\n",
            "         [3., 3., 3., 3., 3.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFKw4ucE9Nzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squash(x):\n",
        "    square_x = (x**2).sum(-1,keepdim = True)\n",
        "    output = (square_x * x)/((square_x) * torch.sqrt(square_x))\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVguZndZqouy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "44d8a6ec-8ddf-4b6b-dc9a-58bdf8c9b69e"
      },
      "source": [
        "test_tensor = torch.ones(1,2,3,2)*2\n",
        "##print(torch.sum(test_tensor,dim = -1))\n",
        "square_test = torch.sqrt((test_tensor**2).sum(-1,keepdim = True))\n",
        "print(test_tensor)\n",
        "print(square_test)\n",
        "print(torch.sum(test_tensor - square_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[2., 2.],\n",
            "          [2., 2.],\n",
            "          [2., 2.]],\n",
            "\n",
            "         [[2., 2.],\n",
            "          [2., 2.],\n",
            "          [2., 2.]]]])\n",
            "tensor([[[[2.8284],\n",
            "          [2.8284],\n",
            "          [2.8284]],\n",
            "\n",
            "         [[2.8284],\n",
            "          [2.8284],\n",
            "          [2.8284]]]])\n",
            "tensor(-9.9411)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBGg9mQ7ttJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_tensor)\n",
        "print(test_tensor **2)\n",
        "print(torch.sqrt(test_tensor ** 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i04e4iYot35F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c93596ee-8758-438d-b3d6-f0567f4d2828"
      },
      "source": [
        "test_tensor = torch.ones(2,2,3,1)\n",
        "output = test_tensor.sum(dim = 2)\n",
        "print(output)\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[3.],\n",
            "         [3.]],\n",
            "\n",
            "        [[3.],\n",
            "         [3.]]])\n",
            "torch.Size([2, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9zz-R-IX9yc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "530140b5-607a-4ce2-b2f6-af06b1b99e18"
      },
      "source": [
        "test_tensor = torch.ones(2,10,1)\n",
        "#print(F.softmax(test_tensor))\n",
        "#print(F.softmax(test_tensor,dim = 1))\n",
        "value, index = test_tensor.max(1)\n",
        "print(index.size())\n",
        "masked = torch.eye(10)\n",
        "masked = masked.index_select(dim = 0,index = index.squeeze(1).data)\n",
        "print(masked.size())\n",
        "print(masked)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 1])\n",
            "torch.Size([2, 10])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugj9ORhdZB30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "afa1d016-08d2-4960-edd9-df316b9bb5a9"
      },
      "source": [
        "print(torch.eye(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_ls5UwLb0Sk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "682da200-105c-4773-be7f-0a96d00ea111"
      },
      "source": [
        "array = torch.ones(1,2)\n",
        "array2 = torch.ones(1,2,3)\n",
        "print(array2 * array[:,:])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d56dbe2b1259>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0marray2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz5w0J5umo7z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "62594d8c-84e9-4903-f2e1-40d95a9399f2"
      },
      "source": [
        "value = 10\n",
        "def change_value(value):\n",
        "    print('Before changing, value is ',value)\n",
        "    value = 20\n",
        "change_value(value)\n",
        "print('After changing value is ',value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before changing, value is  10\n",
            "After changing value is  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW9u1QMTbhYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}